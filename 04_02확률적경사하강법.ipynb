{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNC00i2Kmdded6DRJPmYSG5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swleegit/Hongong_ML/blob/main/04_02%ED%99%95%EB%A5%A0%EC%A0%81%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 확률적 경사 하강법\n",
        "- 실제 데이터는 방대하며, 매일 새로운 데이터가 추가됨.\n",
        "\n",
        "- 기존 데이터 + 새로운 데이터를 통으로 매일 학습 --> 큰 크기의 데이터를 한번에 학습하면 데이터 손실이 있을 수 있으며, 이를 한번에 학습하기 위한 비용도 많이든다.\n",
        "\n",
        "- 전체 데이터의 개수를 일정하게 유지 : 데이터의 양을 조절하는 과정에서 중요 데이터 손실가능성 커짐 .\n",
        "\n",
        "제일 좋은 방법은 기존 데이터로 학습한 것에 이어서 새로 추가되는 데이터에 대해서만 추가학습하여 최종 반영하는 방법 \n",
        "\n",
        "- 이런식의 훈련 방식을 **점진학습** 또는 **온라인 학습**이라고 한다.\n",
        "\n",
        "- 대표적인 점진적학습 알고리즘으로는 **확률적 경사 하강법(Stochastic Gradient Descent)**이 있다.\n",
        "\n",
        "- 당연히 사이킷런에서 확률적 경사 하강법을 위한 클래스(추정기)를 제공한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "3577-2SWfW5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex)새로운 샘플 (50개 가정)  \n",
        "\n",
        "- 확률적 : 50개의 샘플 중 무작위로 한개의 샘플을 골라 이를 이용하여 **학습**한다.  \n",
        "여기서 **학습**이란 산(손실함수)의 현재 위치에서 가장 가파른 경사를 찾아 **조금씩**내려가는 것을 의미한다.  \n",
        "- **조금씩**내려가는 이유는 실제로 산(손실함수)는 상상할 수 없는 복잡한 형상이며, 많이 내려왔다가는 최저의 손실함수 값을 지나칠 수 있기 때문이다.  \n",
        "\n",
        "- 샘플 한개에 대해 하강했다면, 그 다음 샘플에 대해 **이어서** **학습(=하강)한다.** 이렇게 50개의 샘플을 모두 비우면 **1 epoch**한 것.\n",
        "\n",
        "- 이어서 학습하기 때문에 기존 데이터를 통으로 다시 학습 할 필요없이 기존 정보를 유지한체 새로운 데이터에 대한 학습이 가능하다.\n",
        "\n",
        "- 어디까지 하강하는가?(=목표는 무엇인가?)  \n",
        "만족할 수 있는 위치가 목표이다.   \n",
        "산(손실함수)의 최솟값은 알지 못한다. 자세히 말하자면, 이전 샘플과 비교시 일정이상 차이가 없다면 그만둔다.(또는 설정한 epoch가 목표)\n"
      ],
      "metadata": {
        "id": "XSGwduznjjDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 여러개의 샘플을 사용해 경사 하강법 : 미니배치 경사 하강법(minibatch gradient descent)\n",
        "\n",
        "- 전체 샘플을 이용해 경사 하강법 : 배치 경사하강법(batch gradient desent)  \n",
        "모든 데이터를 사용하기 때문에 가장 안정적인 방법이 될 수 있지만, 컴퓨터 자원을 많이 사용하고 데이터가 너무 많으면 한번에 전체 데이터를 모두 읽을 수 없을 수도 있다.\n",
        "  \n",
        "<img src = \"https://drive.google.com/uc?id=1v6qCU9YfNHiZzQcuc0qEDjfyjhBOAWew\">"
      ],
      "metadata": {
        "id": "YHZQ_y1smMHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 손실함수(loss function)\n",
        "- 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지(답(target)과 예측간의 차이)를 측정하는 기준 \n",
        "- 대부분의 머신러닝 알고리즘은 반복을 통해 최적의 모델을 찾으며, 이때 알고리즘이 얼마나 엉터리인지를 나타내는 기준이 필요하다.\n",
        "- 손실함수에는 종류가 많다.\n",
        "\n",
        "손실함수(loss fuction)과 비용함수(cost function)차이  \n",
        "- 손실함수는 샘플 하나에 대한 손실을 정의하고 비용함수는 훈련 세트에 있는 모든 샘플에 대한 손실함수 합을 말한다.\n"
      ],
      "metadata": {
        "id": "0FIFeqgSnm4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1GkGCpcDd4BKsyzbeVxbStwFj9Dt2k-jU\">"
      ],
      "metadata": {
        "id": "1yQIZpzn-0wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위 손실함수를 로지스틱 손실 함수(logistic loss function) 또는 이진 크로스엔트로피 손실함수(binary cross-entropy loss function)이라고 부른다.  \n",
        "\n",
        "- 로지스틱 손실함수를 사용하면 로지스틱회귀모델이 만들어진다.  \n",
        "\n",
        "- 당연히 분류를 위한 다른 손실함수도 있다. 그 중 로지스틱 손실 함수를 배운것임.\n",
        "\n",
        "- 다중분류를 위한 손실함수로는 크로스 엔트로피 손실함수(cross-entropy loss function)이 있다.\n",
        "\n",
        "- 회귀(LineaRegression)의 손실함수로 평균 절댓값 오차를 사용할 수 있다.  \n",
        "평균 절댓값 오차 : 타깃에서 예측을 뺀 절댓값을 모든 샘플에 평균한 값  \n",
        "또는  \n",
        "평균 제곱 오차(mean squared error):  \n",
        "타깃에서 예측을 뺀 값을 제곱한 다음 모든 샘플에 대해(n개의 샘플)에 평균한 값 .\n",
        "\n"
      ],
      "metadata": {
        "id": "pSYeWlK-_QdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGDClassifier\n",
        "\n",
        "- SGDClassifier은 여러 종류의 손실함수를 loss 매개변수에 지정하여 다양한 **분류** 머신러닝 알고리즘을 지원할 수 있다.\n",
        "\n",
        "- SGDClassifier은 **확률적 경사 하강법**을 제공하는 대표적인 분류용 클래스(추정기)\n",
        "\n",
        "- 여태까지 다루었던 sklearn의 퍼셈트론, 로지스틱회귀, SVM은 경사하강법 아록리즘을 적용한 것\n",
        "\n",
        "- 만약 머신러닝을 수행할 데이터가 대용량이라면 확률적 경사하강법이 효율적이다.\n",
        "\n",
        "- sklearn의 SGDClassifier 클래스는 확률적 경사하강법 알고리즘을 적용한 퍼셉트론, 로지스틱 회귀, SVM을 활용할 수 있도록 해준다 \n",
        "\n",
        "\n",
        "매개변수\n",
        "- loss : 확률적 경사 하강법으로 최적화할 손실함수를 지정한다.  \n",
        "\n",
        "- loss = 'hinge'    \n",
        "hinge는 hinge loss(=support vector machine)라 불리는 또 다른 손실함수\n",
        "- loss = 'log' : 이진분류의 경우 로지스틱 손실함수를 사용하며, 이 모델은 로지스틱회귀 모델이 된다.  \n",
        "\n",
        "- loss = 'log' : 다중분류의 경우 크로스엔트로피 손실함수를 사용하며, 일대다(OvR : one versus rest)방식을 사용한다.  \n",
        "일대다 : 어떤 한 클래스를 양성클래수로 두고 나머지를 모두 음성클래스로 두는 방식 \n",
        "\n",
        "- max_iter : epoch횟수를 지정한다. 기본값 1000\n",
        "\n",
        "- tol : 반복을 멈출 조건. n_iter_no_change 매개변수에서 지정한 에포크 동안 손실이 tol만큼 줄어들지 않으면 알고리즘이 중단된다.  \n",
        "tol 매개변수의 기본값은 0.001이고 n_iter_chage 매개변수 기본값은 5다.\n",
        "\n",
        "- penalty : 규제의 종류를 지정할 수 있다.   \n",
        "기본값은 L2 규제를 위한 'l2', L1규제를 적용하려면 'l1'로 지정한다.  \n",
        "규제 강도는 alpha 매개변수에서 지정한다.  \n",
        "기본값은 0.0001\n",
        "\n",
        "LogisticRegression과 비교 \n",
        "\n",
        "[공통점]\n",
        "- l1, l2 규제를 제공한다.(penalty)\n",
        "- 기본적으로 규제 적용가능 모델이기 때문에 표준화 전처리 과정이 필요하다.\n",
        "- 로지스틱 손실함수 사용가능성\n",
        "\n",
        "[SGDClassifier]  \n",
        "- partial_fit 메서드(이어서 확률적 경사하강법으로 추가학습) 제공\n",
        "- log이외에 다른 손실함수(hinge등) 선택지 있음 \n",
        "- alpha로 규제 조정 (alpha 크면 w 작음)\n",
        "- epoch로 과대/과소적합 조정 가능함.  \n",
        "- 다중분류에서 loss = log를 지정하면 '일대다'방식만 지원 가능함.\n",
        "- 미니배치 경사 하강법이나 배치 하강법을 제공하지 않음. 즉, 무조건 샘플 한개씩 꺼내서 학습 \n",
        "\n",
        "- 확률적 경사 하강법이기 때문에 전체적으로는 같은 샘플이더라도 어떤 순서로 뽑냐에 따라서 점수가 달라짐.  \n",
        "따라서 객체생성시 random_state 지정이 결과에 영향을 준다.\n",
        "\n",
        "[LogisticRegression]\n",
        "- solver = lbfgs,sag,saga  \n",
        "sag,saga는 확률적 평균 경사 하강법 사용, 특성과 샘플 수가 많을 때 성능은 빠르고 좋다.  \n",
        "~~아마 확률적 평균 경사하강법은 그냥 확률적 경사 하강법과 차이가 있을 것이다~~\n",
        "\n",
        "- 객체 생성시 random_state에 따라 점수 차이가 없다\n",
        "\n",
        "- **단! partial_fit 제공하지 않음**  \n",
        "- **즉, fit 이후 추가 데이터만 따로 학습하여 반영하는 것이 불가**\n",
        "- **새로운 데이터를 반영하고 싶다면, 기존데이터 + 새로운 데이터를 반영하기 위한 새로운 모델을 만들어야 한다**\n",
        "\n",
        "- multi_class : 다중 분류에서 일대다 방식 뿐만아니라 다양한 방식을 지원한다.\n",
        "\n",
        "- C로 규제의 강도를 제어 : 기본값은 1.0, C클수록 W크다"
      ],
      "metadata": {
        "id": "lmuRhWlDBuaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "fish = pd.read_csv('https://bit.ly/fish_csv_data')\n"
      ],
      "metadata": {
        "id": "JSBiRHvABytc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()\n",
        "fish_target = fish['Species'].to_numpy()"
      ],
      "metadata": {
        "id": "Q-4jP8xYB6tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_input, test_input, train_target, test_target = \\\n",
        "train_test_split(fish_input , fish_target, random_state = 42)\n"
      ],
      "metadata": {
        "id": "qenvrhHECPNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#표준화 전처리(scale조정, z점수로 반환) 변환기중 StandardScaler사용 \n",
        "#훈련세트에서 학습한(fit)한 통계 값으로 테스트 세트도 변환해야한다.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler() # 객체 생성 \n",
        "ss.fit(train_input) #훈련세트 각열의 평균과 표준편차를 구한다.\n",
        "train_scaled = ss.transform(train_input) #훈련세트의 평균과 표준편차로 훈련세트를 표준화한다.\n",
        "test_scaled = ss.transform(test_input)# 훈련세트의 평균과 표준편차로 테스트세트를 표준화한다.\n"
      ],
      "metadata": {
        "id": "nBORXj3qCthO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#사이킷런에서 확률적 경사 하강법을 제공하는 대표적인 분류용 클래스(추정기)는 SGDClassifier다.\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "sc = SGDClassifier(loss = 'log', max_iter = 10, random_state = 42)\n",
        "sc.fit(train_scaled, train_target)\n",
        "print(sc.score(train_scaled, train_target))\n",
        "print(sc.score(test_scaled, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMoAj-EaG_FB",
        "outputId": "335a7a82-1855-4c5a-859e-3da5719f7b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.773109243697479\n",
            "0.775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convergencewarning : 모델이 충분히 수렴하지 못했을 때, max_iter의 값을 늘려주는게 좋다.  \n",
        "SGDClassifier 객체를 다시 만들지 말고 위에서 훈련한 모델 sc에서 추가로 훈련해보자.  \n",
        "비유하자면 산에서 이전에 위치에서 다시 하강 시작한다는 의미"
      ],
      "metadata": {
        "id": "a2KQv6UkHjsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#partial_fit은 fit과 사용법이 같지만 호출할 때마다 1에포크씩 이어서 훈련 가능하다.\n",
        "sc.partial_fit(train_scaled, train_target)\n",
        "print(sc.score(train_scaled, train_target))\n",
        "print(sc.score(test_scaled, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8O8XVd0SDpe",
        "outputId": "be1d0df4-d967-412b-a1e4-dcfca73825c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8151260504201681\n",
            "0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_scaled와 train_target을 한꺼번에 모두 한번에 전달했지만 이 알고리즘(SGD)는 전달한 훈련 세트에서 1개씩 샘플을 꺼내어 경사 하강법을 수행한다."
      ],
      "metadata": {
        "id": "gRf0-fdpS48l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 에포크와 과대/과소적합\n",
        "- 에포크 횟수가 적으면 모델이 훈련 세트를 덜 학습한다.\n",
        "- 에포크 횟수가 충분히 많으면 훈련세트를 완전히 학습한다.\n",
        "\n",
        "- 즉, 적은 에포크 횟수 동안에 훈련한 모델은 훈련세트와 테스트 세트에 잘 맞지 않는 과소적합된 모델일 가능성이 높다.\n",
        "- 반대로 많은 에포크 횟수 동안에 훈련한 모델은 훈련세트에 너무 잘 맞아 테스트 세트에는 오히려 점수가 나쁜 과대적합된 모델  \n",
        "<img src = \"https://drive.google.com/uc?id=1UdZfXEINRgEkQ8jp429-x8rcCKW1gJTA\">\n",
        "- 훈련세트의 정확도는 에포크가 증가할수록 꾸준히 증가하지만, 테스트 세트 점수는 어느 순간 감소한다.\n",
        "- 바로 이 지점이 모델이 과대적합되기 시작하는 곳\n",
        "- 과대적합이 시작하기 전에 훈련을 멈추는 것을 **조기종료(early stopping)**라고한다.\n"
      ],
      "metadata": {
        "id": "GoQVqaZLXnu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fit을 사용하지 않고 partial_fit 메서드만 사용하려면 \n",
        "#훈련 세트에 있는 전체 클래스의 레이블을 partial_fit()메서드에 전달해줘야한다.\n",
        "#위에서는 fit 이후 partial_fit 했기 때문에 전체 클래스의 레이블을 전달할 필요가 없었던 것이다.\n",
        "#partial_fit 1번은 딱 1epoch 진행한다!\n",
        "\n",
        "import numpy as np\n",
        "sc = SGDClassifier(loss = 'log', random_state = 42) #샘플 한개씩 경사하강하기 때문에 전체는 같아도\n",
        "                                                    #학습하는데 샘플의 순서가 중요할 수 있다\n",
        "train_score = []\n",
        "test_score = []\n",
        "classes = np.unique(train_target)"
      ],
      "metadata": {
        "id": "7IbGMRGjab0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#300번의 에포크 동안 훈련을 반복하여 진행 \n",
        "for _ in range(0,300):\n",
        "  sc.partial_fit(train_scaled, train_target, classes = classes)\n",
        "  train_score.append(sc.score(train_scaled, train_target))\n",
        "  test_score.append(sc.score(test_scaled, test_target))\n",
        "\n",
        "# 1번 반복 이후 2번 반복때는 1번 반복했을 때를 이어서하니 2 epoch째!\n",
        "\n",
        "# (_)는 파이썬의 특별한 변수, 나중에 사용하지 않고 그냥 버리는 값을 넣어두는 용도\n",
        "# 여기서는 ()에 299까지 반복횟수를 임시 저장하기 위한 용도로 사용 "
      ],
      "metadata": {
        "id": "PT0M1BB_dIuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_score, label = 'train')\n",
        "plt.plot(test_score, label = 'test')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc = 'best')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "kanlMZSreTxT",
        "outputId": "0e4c1e29-bab3-4021-d89d-d1ae13c8faf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "초반에는 test > train : 과소적합  \n",
        "100 이후 train과 test간격이 벌어짐 : 과대적합방향\n"
      ],
      "metadata": {
        "id": "b7ALImixfBuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SGDClassifier(loss = 'log', max_iter = 100, tol = None, random_state = 42)\n",
        "sc.fit(train_scaled, train_target)\n",
        "print(sc.score(train_scaled, train_target))\n",
        "print(sc.score(test_scaled, test_target))"
      ],
      "metadata": {
        "id": "-FKoI4iYfa6x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7700f581-5948-4b4b-869c-310b5ebf980b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.957983193277311\n",
            "0.925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loss = 'hinge'\n",
        "\n",
        "sc = SGDClassifier(loss = 'hinge', max_iter = 100, tol = None, random_state = 42)\n",
        "sc.fit(train_scaled, train_target)\n",
        "print(sc.score(train_scaled, train_target))\n",
        "print(sc.score(test_scaled, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_JyDgxpK0hb",
        "outputId": "050806c5-c678-43d8-8feb-c17757bdd88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9495798319327731\n",
            "0.925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGDRegressor\n",
        "- 확률적 경사 하강법을 사용한 회귀 모델 \n",
        "- loss 매개변수에서 손실함수를 지정하고, 기본값은 제곱 오차를 나타내는 'squared_loss'다.\n",
        "\n",
        "- 나머지는 SGDClassifier의 매개변수 사용법고 같음."
      ],
      "metadata": {
        "id": "Uq6RpJycPcs3"
      }
    }
  ]
}