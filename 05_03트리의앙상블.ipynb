{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcklMlJR61WPuZt2/40puN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swleegit/Hongong_ML/blob/main/05_03%ED%8A%B8%EB%A6%AC%EC%9D%98%EC%95%99%EC%83%81%EB%B8%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정형, 비정형 데이터\n",
        "- 가장 좋은 알고리즘이 있다고 해서 다른 알고리즘을 배울 필요가 없는 것은 아니다. 보편적으로 성능이 좋아 널리 사용되는 알고리즘이 있지만 문제마다 다를 수 있으며 어떤 알고리즘이 더 뛰어나다고 미리 판단하면 안됨.  \n",
        "\n",
        "[정형데이터(structured data)]\n",
        "- 어떤 구주로 되어있는 데이터 : CSV, 데이터베이스, 엑셀에 저장하기 쉽다.\n",
        "[비정형데이터(unstructured data)]\n",
        "- 데이터 베이스나 엑셀로 표현하기 어려운 것: 텍스트데이터, 디지털카메라로 찍은 사진, 핸드폰으로 듣는 디지털 음악  \n",
        "\n",
        "**cf) 텍스트나 사진을 데이터베이스에 저장 불가?**\n",
        "- 가능함, 보편적으로 안된다는 것, 데이터베이스 중에는 구조적이지 않은 데이터를 저장하는데 편리하도록 발전한것이 많다. : NoSQL 데이터베이스는 엑셀이나 CSV에 담긴 어려운 텍스트나 JSON데이터를 저장하는데 용이  \n",
        "\n",
        "**정형 데이터를 다루는데 가장 뛰어난 성과를 내는 알고리즘 : 앙상블 학습(ensemble learning)**  \n",
        "- 이 알고리즘은 대부분 결정트리를 기반으로 만들어져있다.\n",
        "\n",
        "**비정형 데이터는 어떤 알고리즘을 사용??**  \n",
        "- 신경망 알고리즘. 비정형 데이터는 규칙성을 찾기 어려워 전통적인 머신러닝 방법(여태까지 배운것들)로는 모델을 만들기 까다롭다. 하지만 신경망 알고리즘은 사진을 인식하고 텍스트를 이해하는 모델을 만들 수 있다.  \n"
      ],
      "metadata": {
        "id": "rFHkFqCoKB_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 랜덤 포레스트(Random Forest)  \n",
        "- 앙상블 학습 : 정형데이터(엑셀, 데이터베이스 표현)의 끝판왕 알고리즘 모델(보통 그렇다는것, 항상 그런것은 아님)  \n",
        "- 랜덤 포레스트는 앙상블 학습의 대표 모델 : 안정적인 성능으로, 앙상블 학습을 적용하려하면 가장 먼저 랜덤 포레스트 먼저 적용하자!  \n",
        "- 결정 트리(나무)를 랜덤하게 만들어 결정트리의 숲(만든 결정트리의 묶음)을 만든다. 그리고 각 결정트리의 예측을 사용해 최종 예측을 만든다.    \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1VRwgq1q_mKdzQ8ug1kV3Kt1fqgdSyD2Q\">\n",
        "\n",
        "*cf)머신러닝 라이브러리 중 사이킷런에 구현된 앙상블 학습알고리즘을 기준으로 설명한다. 머신러닝 라이브러리 마다 구현 방식에 조금씩 차이가 있을 수 있다.*  \n",
        "\n",
        "- 랜덤포레스트는 각 트리(나무)를 훈련(적합한 노드를 만드는것 : 앞서 했던것!)하기 위한 데이터를 랜덤하게 만든다.  \n",
        "- 입력한 훈련데이터에서 랜덤하게 샘플을 추출하여 훈련데이터를 만든다. **이때 한 샘플이 중복되어 추출될 수도 있다**\n",
        "- 예를 들어 1000개 가방에서 100개씩 샘플을 뽑는다면, 먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣는다. 이런식으로 계속해서 100개를 가방에서 뽑으면 중복된 샘플을 뽑을 수도 있다.\n",
        "- 이렇게 만들어진 샘플을 **부트스트랩 샘플(bootstrap sample)**이라고 한다.\n",
        "- 기본적으로 부트스트랩 샘플은 훈련세트의 크기와 같게 만든다. 즉, 훈련 세트와 크기가 같다.\n",
        "- 예를들어, 1000개의 샘플이 있는 훈련세트가 있으면 이걸 그대로 활용해서 트리모델(나무)를 만드는게 아니라 중복을 허용해서 샘플링 1000번 해서 훈련세트와 크기가 같은 부트스트랩 샘플을 만들어서 이걸로 트리모델을 훈련한다는 것  \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=15EdJk9pQxMQsgdvUmrUT2JNzX-cfenIf\">  \n",
        "\n",
        "- 또한 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾는다. 분류 모델인 RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근(루트)만큼의 특성을 선택한다. 즉 4개의 특성이 있다면 **노드**마다 2개를 랜덤하게 선택하여 사용한다. 하지만 회귀모델인 RandomForestRegressor은 전체 특성을 사용한다.  \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=11KqWFWYuB1bs4dvLajCGQFgjNm9lP0FT\">  \n",
        "\n",
        "- 사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정트리(나무)를 이런 방식으로 훈련한다. 이 100 개의 집합이 숲을이루을 이룬다는 의미\n",
        "- 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스로 예측으로 삼는다.  \n",
        "\n",
        "**[정리]**\n",
        "- 부트스트랩 샘플로 훈련세트와 똑같은 크기로 샘플링하고 학습시 노드마다 전체특성개수의 제곱근 개수만큼 랜덤으로 특성을 뽑아 진행해서 결정트리 모델 한개(나무 한그루)를 만든다. 이러한 과정을 기본적으로 100번해서 100개의 결정트리 모델을 만들어 그룹화 = 숲을 만든다.\n",
        "- 만약 분류문제, 어떤 샘플이 들어옴. 그러면 각각의 나무(결정트리)의 노드별 질문지를 타다가 리프노드에 도달함 그러면 그때 클래스 비율이 곧 각 클래스별 확률임. 이런식으로 모든 결정트리(나무)에 대해서 해당 샘플에 대해 진행을 함. 그리고 각 결정트리의 각 클래스별 확률을 평균냄(100개의 결정트리). 그 확률을 바탕으로 최종 답(분류)를 진행함!\n",
        "- 랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합된느 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다. (과대적합을 막아주니까). 종종 **기본 매개변수** 설정만으로도 아주 좋은 결과를 낸다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1F4x_cMfKc1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data') #csv -> dataframe\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy() #열선택 -> 배열로 변환\n",
        "target = wine['class'].to_numpy()\n",
        "train_input, test_input, train_target, test_target = train_test_split\\\n",
        "(data, target, test_size = 0.2 , random_state = 42)"
      ],
      "metadata": {
        "id": "zgW9Ma6AO31a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#교차검증, 테스트, 검증세트, k_fold, 분할기, 검증세트 점수 평균, 최고의 매개변수를 찾았으면\n",
        "#검증 + 훈련해서 다시 학습, 그리고 최종으로 테스트 세트로 평가\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_jobs = -1, random_state = 42) #n_jobs = -1 병렬로 사용, 부트스트랩 + 100개의 트리 모델 : 빡셈\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score = True, n_jobs = -1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1pTdh4NTclR",
        "outputId": "4658e31b-68b8-4fcc-adc6-40ee5575ce7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[위의 코드 설명]  \n",
        "- 앙상블 학습에서 가장 대표적인 randomforest분류모델을 선언, 매개변수로는 cpu를 병렬로 사용하기 위한 것과 일관된 점수를 얻기 위한 random_state = 42 : 특성을 랜덤하게 하니까)\n",
        "- 만든 렌덤포레스트 모델을 cross_validate 매개변수로 넣고, train_input, train_target, return_train_score(훈련과 검증세트간의 점수비교를 통해 과대 과소적합을 확인하기 위함) 마찬가지로 병렬\n",
        "- 진행과정:  train이 200개의 샘플이면 그 중 10개의 샘플은 검증 나머지 190개는 훈련, 190개를 부트스트랩해서 샘플 190개인거 100개 만들어 (100개의 트리를 학습해야하니까), 그다음 특성개수의 제곱근 랜덤뽑기해서 각 노드별로 질문 만들어서 모델을 만들어 이런 과정을 100번해서 100개의 결정트리로 이루어진 숲을 만들고 10개의 샘플(검증)을 각각의 트리에 넣어 리프노드에서 클래스별로 확률을 구하고 100개 나무 평균내어 최종 결론 내려 그걸 샘플 수(10번)만큼 하고 정답과 비교해서 정확도로 score 반환해, 이걸 검증과 훈련세트에 대해서도 함. 이제까지가 k-fold에서 5개중 1개 끝남 아직 4개 남음 그렇게 4개를 위와같은 과정으로 하고 나온 테스트, 검증세트 점수 5개를 평균낸게 최종 점수고 이들을 비교해서 과대과소 적합을 따지는 것  \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1jo6a8U-wkh-6AcKmRygk4wkyB4Rm5Tq6\">\n",
        "\n"
      ],
      "metadata": {
        "id": "D3YOtNCnVJIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤 포레스트는 결정 트리의 앙상블이기 떄문에 DecisionTreeClassifier가 제공하는 주요한 매개변수를 모두 제공한다.\n",
        "- criterion, max_depth, max_features, min_sample_split, min_impurity_decrease, min_sampels_leaf 등\n",
        "- 결정트리의 큰 장점 중 하나인 특성 중요도를 계산한다.\n",
        "- 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것이다.\n"
      ],
      "metadata": {
        "id": "eWl3mTNXcJ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(train_input, train_target)\n",
        "# 선 학습후 특성 중요도 파악가능함. 교차검증만으로는 학습이 된게 아님!\n",
        "# 교차검증을 통해 최적의 매개변수를 파악했으면 그 매개변수로 다시 학습 + 검증해서 훈련해야함!\n",
        "# 물론 이과정을 쉽게 하려고 하이퍼 파라미터 튜닝이 있음 .\n",
        "print(rf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piR2MT4rcjBy",
        "outputId": "cd892755-4694-4997-f209-85dc4efcecb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 05-02에서 얻은 결과와 비교해보면 당도의 중요도가 감소하고 알코올 도수와 pH특성의 중요도가 조금 상승함\n",
        "- 이런 이유는 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정트리를 훈련하기 때문, 그 결과 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻음.\n",
        "- 이는 과대적합을 줄이고 일반화 성능을 높이는데 도움이 된다.\n",
        "- **일반 결정트리도 특성을 랜덤하게 선택하지만, 랜덤포레스트는 나무 100그루임. 그만큼 더 랜덤하게 선택함!**"
      ],
      "metadata": {
        "id": "kdw0VM2ecpeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RandomForestClassifier에는 자체적으로 모델을 평가하는 점수를 얻을 수 있다.\n",
        "- 랜덤 포레스트 훈련세트에서 중복을 허용하여 부트스트랩 샘플을 만들어 결정트리를 훈련하는데, 이 때 부트스트랩 샘플에 포함되지 않고 남는 샘플이 있다. 이런 샘플을 **OOB(out of bag)샘플이라고 한다. 이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다.\n",
        "**마치 검증세트의 역할을 하는 것이다**\n",
        "- 즉 교차검증을 사용하지 않고 검증세트를 활용하는 효과를 얻는것\n",
        "- 200개의 훈련 샘플을 부트스트랩 샘플, 이과정을 100번 반복, 100번해서 얻어진 샘플 집합들로 100개의 트리를 학습시켜 1개의 숲인 랜덤포레스트를 만들어. 그리고 **나무 한개를 만드는 과정에서 뽑히지 않은 샘플로 방금 만든 나무 한개에 대한 검증을함** 이렇게 검증 **100번**한 점수를 평균하여 OOB 점수를 출력함 --> 검증세트 역할\n",
        "- 이 점수를 얻으려면 RandomForestClassifer클래스의 oob_score = True로 지정(기본값은 False) 이렇게 하면 랜덤 포레스트는 각 결정트리의 OOB점수를 평균하여 출력한다."
      ],
      "metadata": {
        "id": "0cvCZGxOdYuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 42)\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aoGucm-ecnv",
        "outputId": "f0eaef1a-a996-46b4-c829-312d0ff47b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 교차 검증에서 얻은 점수와 매우 비슷한 결과를 얻음.\n",
        "- oob점수를 사용하면 교차 검증을 대신할 수 있어서 결과적으로 훈련세트에 더 많은 샘플을 사용할 수 있다."
      ],
      "metadata": {
        "id": "KCn3hGaAe7IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 엑스트라 트리(Extra Tree)\n",
        "- 랜덤 포레스트와 매우 비슷하게 동작\n",
        "- 기본적으로 100개의 결정트리를 훈련, 랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수를 지원한다.\n",
        "- 또한 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용한다.\n",
        "- 랜덤 포레스트와 엑스트라 트리의 차이점은 부트스트랩 샘플을 사용하지 않고 전체훈련세트를 사용한다.\n",
        "- 디신 노드를 분할 할 때 가장 좋은 분할을 찾는 것이 아니라(=정보 이득이 크도록 하는것이 아니라= 부모와 자식노드간의 불순도 차이가 크도록 하는 것이 아니라) **무작위로**분할한다.\n",
        "- 하나의 나무를 생성시 DecisionTreeClassifier의 splitter 를 random으로 지정하는 것으로 생각\n",
        "- 하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만(불순도가 작게 날카로운 질문을 해야하는데 그게 아니라 그냥 자기 맘대로 질문을 만들어 버리니까) 많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증세트의 점수를 높이는 효과가 있다."
      ],
      "metadata": {
        "id": "Plb_yLFrfHkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "et = ExtraTreesClassifier(n_jobs = -1, random_state = 42)\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score = True, n_jobs = -1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2hj_Aygxw_2",
        "outputId": "d899b905-b5c9-4741-ee7d-fab04d22ffa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야한다. 하지만 랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 엑스트라 트리의 장점이다. (기존의 것은 정보이득을 크게하기 위한 질문을 계산하느라 시간이 많이 걸림, 특히 고려해야할 특성의 개수가 많을 때 더 그렇다)\n",
        "- 엑스트라 트리도 랜덤 포레스트와 마찬가지로 특성 중요도를 제공한다.\n",
        "- 엑스트라 트리의 회귀 버전은 ExtraTreesRegressor"
      ],
      "metadata": {
        "id": "Xf9S1aRqy4Pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd6JG3RIzWg-",
        "outputId": "cc968907-3d15-4048-87a7-2d08c552d244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 그레디언트 부스팅(gradient boosting)\n",
        "- 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다.\n",
        "- 사이킷런의 GradientBoostingClassifier은 기본적으로 깊이가 3인 결정트리를 100개를 사용한다. 깊이가 얕은 결정트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.\n",
        "- 4장에서 배웠던 경사하강법을 사용하여 결정트리를 앙상블(숲)에 순차적으로 추가한다.\n",
        "- 분류에서는 로지스틱 손실함수를 사용하고 회귀에서는 평균 제곱 오차 함수를 사용한다\n",
        "  - 손실함수 : 정답과 예측의 차이를 나타내고, 예측이 정답과 멀수록 그 값은 크다.\n",
        "- 4장에서 경사 하강법은 손실함수를 산으로 정의하고 가장 낮은 곳을 찾아 내려오는 과정으로 설명함.   \n",
        "이때 가장 낮은 곳을 찾아 내려오는 방법은 모델의 가중치와 절편을 조금씩 바꾸는 것, 그리고 손실함수의 낮은 곳으로 천천히 조금씩 이동해야한다.  \n",
        "- 이처럼 그레이디언트 부스팅도 깊이 3까지만 진행하여 한개의 트리를 만들고 (분류라면)로지스틱 손실함수를 통해 정답과 예측의 차이를 나타내고, 그 값이 작아지도록 다음 트리를 순차적으로 만든다. 여기서 깊이가 3인 트리밖에 안하는 이유는 4장과 같이 천천히 내려오기 위함임.\n",
        "- 또한 학습률 매개변수로 속도를 조절할 수 있다."
      ],
      "metadata": {
        "id": "GSRu91_zzqBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb = GradientBoostingClassifier(random_state = 42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs = -1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAzOGFFD-pL8",
        "outputId": "797bca3a-0bb4-4c06-cc85-b7ec2a06f1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 그레이디언트 부스팅은 결정트리의 개수를 늘려도 과대적합에 매우 강하다. 학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있다."
      ],
      "metadata": {
        "id": "XA6eUIwg_C_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.2, random_state = 42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs = -1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyoO7cH5_Lgw",
        "outputId": "686f41e1-4760-4ad6-8946-4414a2951e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결정 트리 개수를 500개로 5배나 늘렸지만 과대적합을 잘 억제하고 있다. 학습률 learning_rate = 0.1(기본값) **학습률의 정확한의미??**\n",
        "\n",
        "-그레디언트 부스팅도 특성 중요도를 제공한다."
      ],
      "metadata": {
        "id": "9qlGsNNyA3Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v_XC8arBJd_",
        "outputId": "05f48c71-a53f-4c75-854e-4cc6db0e69be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 트리 훈련에 사용할 훈련세트 비율을 정하는 subsample.\n",
        "- 이 매개변수의 기본값은 1.0으로 전체 훈련세트를 사용한다. 하지만 subsample이 1보다 작으면 훈련세트의 일부를 사용한다.\n",
        "- 이는 마치 경사하강법 단계마다 일부 샘플을 랜덤하게 선택하여 진행하는 확률적 경사하강법이나 미니배치 경사하강법과 비슷하다.\n",
        "- 일반적으로 그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있다. 하지만 순서대로 트리를 추가하기 때문에 훈련 속도가 느리다.\n",
        "- 즉 GradientBoostingClassifier에는 n_jobs 매개변수가 없다(위에는 cross_validate의 n_jobs).\n",
        "- 그레이디언트 부스팅의 회귀 버전은 GradientBoostingRegressor\n",
        "\n",
        "[정리]\n",
        "- 얕은 깊이의 트리(조금씩 내려가기 위함)을 숲에 순차적으로 추가함. 추가할때 손실함수가 가장 낮은 곳을 찾아 이동함.(분류 : 로지스틱 손실함수, 회귀에서 평균제곱오차함수, 손실함수 : 예측과 정답 차이를 나타내는 함수), 손실함수 값이 낮은 곳을 찾아 내려오는 방법은 (다음트리를 만드는 방법은) 모델의 가중치와 절편을 조금씩 바꾸는 것\n",
        "- 랜덤포레스트보다 일반적으로 성능은 좋지만 순차적으로 진행하기 때문에 속도가 느린 편(병렬사용 불가)\n",
        "- 결정트리의 개수가 늘어나도 과대적합이 쉽게 되질 않는다. (깊이가 얕기 때문에)\n",
        "- **학습률(?)매개변수로 속도를 조절할 수 있다.??**\n",
        "- subsample 매개변수로 미니배치 경사하강법처럼 사용가능함.\n",
        "\n"
      ],
      "metadata": {
        "id": "U0ST9nTXBPjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)\n",
        "- 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘\n",
        "- 히스토그램 기반 그레이디언트 부스팅은 먼저 입력 특성의 값을 256개의 구간으로 나눈다. 학습시 제한된 구한을 활용하기 때문에 노드를 분할할때 최적의 분할을 매우 빠르게 찾을 수 있다.\n",
        "- 그리고 256개의 구간 중에서 하나의 구간을 떼어놓고 누락된 값을 위해서 사용한다. 따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요 없다.\n",
        "   - 원래 누락된 값이 있으면 해당특성의 중요값 또는 평균 또는 최빈값등으로 채워야하는 전처리 과정이 필요한다 히스토 그램 기반 그레이디언트 부스팅 사용시 이런 과정이 필요없다.\n",
        "- 사이킷런의 히스토그램 기반 그레이디언트 부스팅 클래스는 HisGradientBoostingClassifier이다.\n",
        "- 일반적으로 해당 클래스에서는 기본 매개변수(디폴트값)에서 안정적인 성능을 얻을 수 있다.\n",
        "- 그냥 그레이디언트 부스팅이나 이거나 경사하강법을 진행함. 앞서 그레이디언트 부스팅은 나무 개수 3개, n_estimators 매개변수를 이용함 .\n",
        "- 히스토는 트리의 개수를 지정하는데 n_estimators 대신에 부스팅 반복횟수를 지정하는 max_iter을 사용한다.\n",
        "- 성능을 높이려면 max_iter매개변수를 테스트해보자.\n"
      ],
      "metadata": {
        "id": "E_dClkyUDfCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting  #아직은 실험단계여서 해당 모듈을 임포트\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "hgb = HistGradientBoostingClassifier(random_state = 42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score = True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "id": "BR_9uLk3Dk1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397bb02f-405c-4a4d-a0e0-e61aab10422a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 히스토그램 기반 그레이디언트 부스팅의 특성 중요도를 계산하기 위해 permutation_importance() 함수를 사용.\n",
        "- 이 함수는 특성을 하나씩 그 특성의 값들을 랜덤하게 섞어서 모델의 성능이 어떻게 변화하는지를 관찰하여 어떤 특성이 중요한지를 계산한다.\n",
        "  - a,b,c 특성이 있을 때 a특성의 값을 랜덤하게 섞어, 그렇다고 연관된 bc가 그에 따라 같이 섞이는게 아님. 한마디로 엉망진창.. 그렇게 해서 성능을 구하고 나머지 특성에 대해서도 섞고 성능을 구해. 그다음 정상적인 친구 성능을 구해서 비교함 .\n",
        "  - 만약 a특성을 섞었을 때 점수 70 b 특성을 섞었을 때 60 정상일때 90이면 b특성이 중요한거겠지, 중요한 특성을 엉망으로 만들면 오히려 점수가 안나오는 건 당연함\n",
        "- 훈련세트뿐만 아니라 테스트 세트에도 적용할 수 있고 사이킷런에서 제공하는 추정기 모델(여태까지 배운것들 estimator)에 모두 사용할 수 있다.\n",
        "- permutation_importace로 특성의 중요도를 판단하는 것을 권장함!"
      ],
      "metadata": {
        "id": "ZGCI0t89y3n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "hgb.fit(train_input, train_target)\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats = 10, random_state = 42, n_jobs = -1)\n",
        "#n_repeat : 랜덤하게 섞을 횟수를 지정, 기본값 5\n",
        "print(result.importances_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F3g-rxMzeT1",
        "outputId": "651472af-d79d-427e-ca06-42f4115279a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- permutation_importance()함수가 반환하는 객체는 반복하여 얻은 특성의 중요도(importances), 평균(importances_mean), 표준편차(importaces_std)  \n",
        "- 10번 반복하면 각 반복마다 성능을 구하고 정상과 비교해서 특성의 중요도 산출, 그럼 중요도가 10개 나오겠지, 그 값들의 평균 또는 표준편차를 의미하는 것\n"
      ],
      "metadata": {
        "id": "IxYX0lCC0bbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#이번에는 테스트 세트에서 특성 중요도를 계산해보자.\n",
        "\n",
        "result = permutation_importance(hgb, test_input, test_target, n_repeats = 10,\\\n",
        "                               random_state = 42, n_jobs = -1)\n",
        "print(result.importances_mean)\n",
        "\n",
        "#이런 분석을 통해 모델을 실전에 투입했을 때 어떤 특성에 관심을 둘지 예상할 수 있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JRJ7YjJ3ZNw",
        "outputId": "d7b0c6aa-583b-4c52-f1a6-94ac23587fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#성능 최종 확인\n",
        "hgb.score(test_input, test_target)"
      ],
      "metadata": {
        "id": "pWI6vjZV33P0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd64afc5-767c-4785-bbe8-f49339866168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 히스토그램 기반 그레이디언트 부스팅의 회귀 버전은 HistGradientBoostingRegressor 클래스에 구현되어 있다.\n",
        "- 사이킷런 말고도 그레이디언트 부스팅 알고리즘을 구현한 라이브러리가 여렀있다. (XGBoost, LightGBM)"
      ],
      "metadata": {
        "id": "5pjqrzzX4H2a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7a5kjHO4fQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}